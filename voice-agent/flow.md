# Voice Agent Flow Documentation

A detailed breakdown of how the voice agent processes audio through multiple AI services.

## ðŸš€ Complete Runtime Execution Flow

Here's a step-by-step flow of how the code executes, sorted for learning the runtime flow of this Python voice agent program:

### âœ… 1. File Starts Executing

```python
if __name__ == "__main__":
    asyncio.run(main())
```

- Python checks if this is the main script and starts running it
- It runs the `main()` function asynchronously using `asyncio.run`

### âœ… 2. main() Function Begins

```python
async def main():
```

**Inside main():**

#### ðŸŸ¢ a. Load Environment Variables

```python
load_dotenv()
```

#### ðŸŸ¢ b. Check for API Key

```python
if not os.getenv("OPENAI_API_KEY"):
    # If missing, show error and stop
```

#### ðŸŸ¢ c. Create an Agent

```python
agent = SimpleVoiceAgent(...)
```

This initializes:

- OpenAI connectors
- Audio recorder
- Audio player
- System prompt and conversation history

### âœ… 3. SimpleVoiceAgent **init**() Executes

```python
class SimpleVoiceAgent:
    def __init__(...):
```

**Loads:**

- `OpenAIChatCompletion`
- `OpenAIAudioToText`
- `OpenAITextToAudio`

**Creates:**

- `AudioRecorder`
- `AudioPlayer`

**Adds the system prompt to conversation history.**

### âœ… 4. Enter Conversation Loop

```python
await agent.conversation_loop()
```

**Inside conversation_loop():**

#### ðŸ” LOOP BEGINS:

User sees:

```
Press Enter to start recording (or type 'exit' to quit):
```

### âœ… 5. If Enter is Pressed â†’ Starts Recording

```python
user_input = await self.record_and_transcribe()
```

**Inside record_and_transcribe():**

#### ðŸŸ¡ a. Record for 5 seconds

```python
self.recorder.record_audio(duration=5)
```

Uses `AudioRecorder.start_recording()` and reads audio chunks.

#### ðŸŸ¡ b. Save audio to temp file

```python
self.recorder.save_audio(temp_filename)
```

#### ðŸŸ¡ c. Transcribe via Whisper

```python
audio_content = AudioContent.from_audio_file(temp_filename)
text_content = await self.audio_to_text.get_text_content(audio_content)
```

### âœ… 6. Display & Process User Input

```python
response = await self.get_ai_response(user_input)
```

**Inside get_ai_response():**

- Adds user message to history
- Calls OpenAI GPT (`get_chat_message_content()`)
- Adds assistant reply to history

### âœ… 7. Text-to-Speech

```python
await self.speak_response(response)
```

**Inside speak_response():**

- Calls OpenAI TTS (`get_audio_content()`)
- Plays audio using `AudioPlayer.play_audio()`

### ðŸ” 8. Loop Repeats Until Exit

If user types or says "exit" â†’ exits the loop.

### âœ… 9. Clean Up Resources

```python
agent.cleanup()
```

- Calls `self.recorder.cleanup()` â†’ releases PyAudio mic
- Calls `self.player.cleanup()` â†’ releases PyAudio speaker

---

## ðŸ”„ FULL EXECUTION FLOW SUMMARY:

```
main() â†’
    load_dotenv() â†’
    create SimpleVoiceAgent â†’
        init chat/audio models + PyAudio â†’
        add system prompt to history
    â†’
    conversation_loop() â†’
        wait for user input â†’
        record_and_transcribe() â†’
            record audio â†’
            save .wav â†’
            transcribe to text
        â†’
        get_ai_response() â†’
            get GPT response
        â†’
        speak_response() â†’
            TTS â†’ play response
        â†’
        repeat or exit â†’
    cleanup()
```

---

## ðŸŽ¯ Individual Component Flows

### ðŸŽ™ï¸ Step 1: Audio Recording (5 seconds)

**Function**: `self.recorder.record_audio(duration=5)`

When you press Enter:

- PyAudio opens the microphone
- It reads short chunks of data in a loop and stores them in `self.frames`
- After 5 seconds, the recording is stopped

**ðŸ“ Code:**

```python
for _ in range(0, int(self.sample_rate / self.chunk_size * duration)):
    data = self.stream.read(self.chunk_size)
    self.frames.append(data)
```

### ðŸ’¾ Step 2: Audio Saved to Temporary File

**Function**: `self.recorder.save_audio(temp_filename)`

- It writes `self.frames` (list of audio chunks) to a `.wav` file
- Used for sending to OpenAI Whisper

### ðŸ§  Step 3: Speech Transcribed by Whisper (STT)

**Function**: `text_content = await self.audio_to_text.get_text_content(audio_content)`

- Converts `.wav` to `AudioContent` object
- Sends it to OpenAI Whisper model (`whisper-1`)
- Whisper returns the text you spoke

**ðŸ“ Code:**

```python
audio_content = AudioContent.from_audio_file(temp_filename)
text_content = await self.audio_to_text.get_text_content(audio_content)
```

**âœ… Result**: Now we have your input as text (e.g., "What's the weather today?")

### ðŸ’¬ Step 4: Text Sent to GPT (Chat Completion)

**Function**: `response = await self.chat_service.get_chat_message_content(...)`

- Your transcribed text is added to the conversation `ChatHistory`
- A chat completion request is sent to GPT (`gpt-4o-mini`)
- The model generates a response (e.g., "It's sunny and 28Â°C today.")
- Response is added to the chat history

**ðŸ“ Code:**

```python
self.history.add_user_message(user_input)
response = await self.chat_service.get_chat_message_content(...)
self.history.add_assistant_message(response.content)
```

**âœ… Result**: Now we have the assistant's reply text.

### ðŸ”Š Step 5: GPT Reply Sent to TTS (Text-to-Speech)

**Function**: `audio_content = await self.text_to_audio.get_audio_content(text, ...)`

- GPT response is sent to OpenAI TTS model (`tts-1`)
- TTS returns audio content (e.g., WAV data of the assistant saying: "It's sunny and 28Â°C today.")

**ðŸ“ Code:**

```python
audio_content = await self.text_to_audio.get_audio_content(
    text,
    OpenAITextToAudioExecutionSettings(response_format="wav")
)
```

**âœ… Result**: We now have audio data ready to play.

### â–¶ï¸ Step 6: Audio Played Back to User

**Function**: `self.player.play_audio(audio_content.data)`

- Uses PyAudio to open a speaker stream
- Plays the audio data returned by TTS

**ðŸ“ Code:**

```python
stream.write(audio_data)
```

**âœ… You now hear the assistant reply out loud.**

### ðŸ” Step 7: Back to the Loop

Once playback is done, it goes back to waiting for the next Enter key press. You can:

- Speak again
- Say "exit" to quit

---

## ðŸ“Š Flow Summary

```mermaid
flowchart TD
    A[Press Enter] --> B[Record Audio 5s]
    B --> C[Save as WAV]
    C --> D[Whisper: Speech â†’ Text]
    D --> E[ChatGPT: Text â†’ Reply]
    E --> F[TTS: Reply â†’ Audio]
    F --> G[Play Audio]
    G --> H[Wait for next interaction]
    H --> A

    style A fill:#e1f5fe
    style D fill:#f3e5f5
    style E fill:#e8f5e8
    style F fill:#fff3e0
```

## ðŸ”§ Technical Details

### AI Models Used:

- **Whisper-1**: Speech-to-Text conversion
- **GPT-4o-mini**: Natural language processing and response generation
- **TTS-1**: Text-to-Speech conversion

### Audio Processing:

- **Sample Rate**: 16,000 Hz
- **Channels**: 1 (mono)
- **Format**: WAV
- **Duration**: 5 seconds per recording

### Error Handling:

- Temporary file cleanup after processing
- Graceful error messages for failed transcription
- Fallback to text output if audio playback fails

### Memory Management:

- **Chat History**: Stored in RAM using Semantic Kernel's ChatHistory
- **Audio Files**: Temporary WAV files (deleted after processing)
- **Session State**: In-memory only (lost on program restart)
